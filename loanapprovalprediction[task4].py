# -*- coding: utf-8 -*-
"""LoanApprovalPrediction[Task4].ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fo_pgFq6ClTIo2x2YQKNvZVJF0YZLhoq
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report, f1_score, precision_score, recall_score
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('/content/loan_approval_dataset.csv')
df.head(10)

df.info()

df.isnull().sum().sum()

df.duplicated().sum()

df.describe().T

# Remove spaces from column names
df.columns = df.columns.str.strip()
df.head()

catigorical_cols = [col for col in df.columns if df[col].dtype == 'O']
for col in catigorical_cols:
    print(f'{col} : {df[col].unique()}')

for col in catigorical_cols:
  print(df[col].value_counts()/len(df)*100)
  print()

"""# **EDA**"""

plt.figure(figsize=(10, 6))
sns.countplot(x='loan_status', data=df, color='skyblue')
plt.title('Loan Status Distribution')
plt.xlabel('Loan Status')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='education', data=df, hue='loan_status', palette='viridis')
plt.title('Education Distribution by Loan Status')
plt.xlabel('Education')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(x='no_of_dependents', data=df, hue='loan_status', palette='coolwarm')
plt.title('Number of Dependents by Loan Status')
plt.xlabel('Number of Dependents')
plt.ylabel('Count')
plt.show()

plt.figure(figsize=(12, 6))
sns.histplot(df['loan_amount'], kde=True, color='skyblue')
plt.title('Distribution of Loan Amount')
plt.xlabel('Loan Amount')
plt.ylabel('Frequency')
plt.show()

numerical_cols = ['no_of_dependents', 'income_annum', 'loan_amount', 'loan_term', 'cibil_score', 'residential_assets_value', 'commercial_assets_value', 'luxury_assets_value', 'bank_asset_value']

plt.figure(figsize=(15, 10))
for i, col in enumerate(numerical_cols):
    plt.subplot(3, 3, i + 1)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
    plt.ylabel(col)

plt.tight_layout()
plt.show()

def check_balance(y):
    counts = y.value_counts()
    total = len(y)
    print("Class distribution:")
    print(counts)
    print("\nClass percentages:")
    print((counts / total * 100).round(2).astype(str) + '%')

    # Rule of thumb: if one class < 40% or > 60%, it's imbalanced
    ratio = counts.min() / counts.max()
    if ratio < 0.8:
        print("\n The dataset is imbalanced.")
    else:
        print("\n The dataset is relatively balanced.")

check_balance(df['loan_status'])

df = df.drop('loan_id', axis=1)
df.head()

"""# **Using LabelEncoder**"""

le = LabelEncoder()
for col in catigorical_cols:
  df[col] = le.fit_transform(df[col])

df.head()

plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot = True, cmap = 'plasma')
plt.title('Correlation Heatmap')
plt.show()

df['debt_to_income'] = df['loan_amount'] / df['income_annum']
df= df.drop(columns=['loan_amount','income_annum'])

corr = df.corr()
plt.figure(figsize=(12,8))
sns.heatmap(corr, annot=True, cmap='coolwarm')

"""# **Scaling by Using [MinMaxScaler]**"""

numerical_col = df.select_dtypes(include=['int64','float64']).columns
scaler = MinMaxScaler()
df[numerical_col] = scaler.fit_transform(df[numerical_col])
df.head()

"""# **Separate and Split data**"""

#Separate
X = df.drop(columns=['loan_status'])
y = df['loan_status']

#Splitting
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# **After Splitting Data Using Smote Method To Balance Data**"""

from imblearn.over_sampling import SMOTE
from collections import Counter

# Before SMOTE
print("Before SMOTE class distribution:", Counter(y_train))

# Create SMOTE instance
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(x_train, y_train)

# After SMOTE
print("After SMOTE class distribution:", Counter(y_train_res))

"""# **LogisticRegression Algorithm**"""

lr_model = LogisticRegression(max_iter=1000)
lr_model.fit(X_train_res, y_train_res)

cv_scores = cross_val_score(lr_model, X_train_res, y_train_res, cv=5, scoring='accuracy')
print("Cross-validation scores:", cv_scores)
print("Mean CV accuracy:", cv_scores.mean(),'\n')

# Train model on full training data
lr_model.fit(X_train_res, y_train_res)

# Evaluate on test data
preds = lr_model.predict(x_test)
print("Test Accuracy:", accuracy_score(y_test, preds))

cm = confusion_matrix(y_test, preds)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Reds')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
print()
print(classification_report(y_test, preds))

"""# **DecisionTreeClassifier Algorithm**"""

dt_model = DecisionTreeClassifier(criterion='entropy')
dt_model.fit(X_train_res, y_train_res)

cv_scores = cross_val_score(dt_model, X_train_res, y_train_res, cv=5, scoring='accuracy')
print("Cross-validation scores:", cv_scores)
print("Mean CV accuracy:", cv_scores.mean(),'\n')

# Train model on full training data
dt_model.fit(X_train_res, y_train_res)

# Evaluate on test data
predd = dt_model.predict(x_test)
print("Test Accuracy:", accuracy_score(y_test, predd))

cm = confusion_matrix(y_test, predd)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Oranges')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
print()
print(classification_report(y_test, predd))

"""# **RandomForestClassifier Algorithm**"""

rf_model = RandomForestClassifier(n_estimators=100)
rf_model.fit(X_train_res, y_train_res)

cv_scores = cross_val_score(rf_model, X_train_res, y_train_res, cv=5, scoring='accuracy')
print("Cross-validation scores:", cv_scores)
print("Mean CV accuracy:", cv_scores.mean(),'\n')

# Train model on full training data
dt_model.fit(X_train_res, y_train_res)

# Evaluate on test data
pred = dt_model.predict(x_test)
print("Test Accuracy:", accuracy_score(y_test, pred))

cm = confusion_matrix(y_test, pred)
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Greens')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()
print()
print(classification_report(y_test, pred))

"""# **Comparison Between DecisionTree and LogisticRegression**"""

def get_metrics(y_true, y_pred):
    return {
        "Accuracy": accuracy_score(y_true, y_pred),
        "Precision": precision_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "F1-Score": f1_score(y_true, y_pred)
    }

# --- Store results ---
results = pd.DataFrame({
    "Logistic Regression": get_metrics(y_test, preds),
    "Decision Tree": get_metrics(y_test, predd)
})

print("\nModel Comparison:")
print(results.round(4))

